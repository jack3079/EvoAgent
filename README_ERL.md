# ğŸ§¬ EvoAgent v2.0 â€” Experiential Reinforcement Learning Edition

> **An AI agent that learns how to THINK, not just what tools to use**

Based on the research paper: ["Kolb-Based Experiential Learning for Generalist Agents with Human-Level Kaggle Data Science Performance"](https://arxiv.org/abs/...)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org)

---

## ğŸ¯ What's New in v2.0

This version implements **Experiential Reinforcement Learning (ERL)**, the approach that powered Agent K to achieve:
- **Top 2% on Kaggle** (1694 Elo-MMR, beyond median human performance)
- **9 gold, 8 silver, 12 bronze medals** in data science competitions
- First AI system to successfully integrate **Kolb's Experiential Learning Cycle**

---

## ğŸ”¬ Two Architectures Included

### 1. Basic EvoAgent (`evolve.py`)
**Learning Type:** Tool accumulation  
**Process:** Identify gap â†’ generate tool â†’ test â†’ integrate

```python
python evolve.py              # Run basic tool evolution demo
python evolve.py --live       # Use real LLM
```

**What it learns:** Individual capabilities (tools)  
**Use case:** Building a comprehensive tool library

---

### 2. ERL Agent (`evolve_erl.py`) â€” â­ The Breakthrough
**Learning Type:** Policy refinement  
**Process:** Attempt â†’ reflect â†’ revise â†’ internalize principle

```python
python evolve_erl.py          # Run ERL demo
python compare_agents.py      # See the difference explained
```

**What it learns:** Reasoning patterns and principles  
**Use case:** Achieving human-level performance on complex tasks

---

## ğŸ’¡ The Key Difference

### Tool-Only Learning (v1.0)
```
Task: Parse CSV â†’ Fails
Agent: "I need a CSV parser"
â†’ Generates csv_parser tool
â†’ Next CSV task: Success âœ“
â†’ JSON task: Still fails âœ—
```
**Result:** Learned 1 capability

### ERL Learning (v2.0)
```
Task: Parse CSV â†’ Fails
Agent: Deep reflection...
  â€¢ "I assumed format without validation"
  â€¢ "I should inspect structure first"
â†’ Principle: "Always validate data structure before processing"
â†’ Internalizes into base reasoning policy
â†’ Next CSV task: Success âœ“
â†’ JSON task: Success âœ“ (applies same principle)
â†’ XML task: Success âœ“
â†’ New format: Success âœ“
```
**Result:** Learned 1 principle that applies to 100+ scenarios

---

## ğŸ§  How ERL Works (Kolb's Cycle)

```
1. Concrete Experience
   â””â”€> Agent attempts task with current policy
        â””â”€> Fails or succeeds suboptimally

2. Reflective Observation
   â””â”€> Deep analysis: "What was my reasoning error?"
        Not: "I need a tool"
        But: "I made incorrect assumption X"

3. Abstract Conceptualization
   â””â”€> Extract generalizable principle
        "When facing X situation, apply Y reasoning"

4. Active Experimentation
   â””â”€> Revised attempt guided by reflection
        â””â”€> Success â†’ internalize into policy
```

---

## ğŸ“Š Performance Comparison

| Metric | Tool-Only | ERL |
|--------|-----------|-----|
| **Learning per task** | 1 tool | 1 principle |
| **Transfer to new domains** | No | Yes |
| **Generalization** | Low | High |
| **Approach** | Capability expansion | Reasoning refinement |
| **Path to AGI** | Uncertain | Clear |

Real-world validation:
- **Agent K (paper)**: Top 2% Kaggle with ERL
- **GPT-4 baseline**: Median performance without ERL

---

## ğŸš€ Quick Start

```bash
# Install
git clone https://github.com/your-username/evoagent.git
cd evoagent
pip install -r requirements.txt

# Run offline demos (no API key needed)
python evolve.py              # Basic evolution
python evolve_erl.py          # ERL evolution
python compare_agents.py      # See the difference

# Run with real LLM
export ANTHROPIC_API_KEY=your_key
python evolve_erl.py --live

# Interactive mode
python main.py
```

---

## ğŸ“ Architecture

```
evoagent/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ agent.py              # Basic agent (tool evolution)
â”‚   â”œâ”€â”€ erl_agent.py          # ERL agent (policy evolution) â­
â”‚   â”œâ”€â”€ policy_store.py       # Learned reasoning principles â­
â”‚   â”œâ”€â”€ reflection_engine.py  # Deep reflection (Kolb's cycle) â­
â”‚   â”œâ”€â”€ code_writer.py        # LLM-driven code generation
â”‚   â”œâ”€â”€ executor.py           # Safe subprocess execution
â”‚   â”œâ”€â”€ memory.py             # Experience & tool storage
â”‚   â””â”€â”€ mock_llm.py           # Offline demo mode
â”‚
â”œâ”€â”€ evolve.py                 # Basic evolution demo
â”œâ”€â”€ evolve_erl.py             # ERL evolution demo â­
â””â”€â”€ compare_agents.py         # Comparison explainer â­
```

---

## ğŸ”‘ Key Components

### PolicyStore (`core/policy_store.py`)
Stores **learned reasoning principles**, not tools.

Example principles:
- "Before processing structured data, validate format first"
- "When assumptions fail, break task into smaller steps"
- "For collection operations, verify non-empty before proceeding"

These are **injected into prompts dynamically** based on task relevance.

### ReflectionEngine (`core/reflection_engine.py`)
Implements deep reflection following Kolb's cycle.

Not: "I need tool X"  
But: "My reasoning error was Y, the principle I should learn is Z"

### ERLAgent (`core/erl_agent.py`)
Main agent implementing two-attempt mechanism:
1. First attempt with current policy
2. If fails â†’ deep reflect â†’ extract principle
3. Second attempt guided by reflection
4. If succeeds â†’ **internalize principle into policy**

---

## ğŸ“ Research Context

This implementation is inspired by:

**Paper:** "Kolb-Based Experiential Learning for Generalist Agents with Human-Level Kaggle Data Science Performance"

**Key innovation:** Converting environmental feedback into durable behavioral improvements through experiential learning, not just capability expansion.

**Agent K achievements:**
- 1694 Elo-MMR (top 2% of 5M+ Kaggle users)
- 9 gold medals in data science competitions
- First AI to successfully integrate cognitive learning theory

---

## ğŸ¤ Contributing

We welcome contributions in:

1. **Better reflection prompts** â€” improve principle extraction
2. **Policy ranking** â€” smarter retrieval of relevant principles
3. **Multi-agent ERL** â€” agents that learn from each other
4. **Benchmarks** â€” measure ERL vs. baseline on standard tasks
5. **Domain-specific principles** â€” bootstrap policies for specialized domains

See [CONTRIBUTING.md](CONTRIBUTING.md)

---

## ğŸ—ºï¸ Roadmap

- [x] v1.0 â€” Basic tool evolution
- [x] v2.0 â€” ERL with policy internalization
- [ ] v2.1 â€” Semantic principle search (embeddings)
- [ ] v2.2 â€” Multi-agent collaborative learning
- [ ] v2.3 â€” Automatic benchmark suite
- [ ] v3.0 â€” Full Kaggle competition agent

---

## ğŸ“– Learn More

**Read the comparison:**
```bash
python compare_agents.py
```

**Watch it learn:**
```bash
python evolve_erl.py
```

The system will show you:
- Initial bootstrap principles
- Tasks that trigger reflection
- Principles extracted from experience
- How principles guide future attempts

---

## âš–ï¸ Philosophy

**Tool evolution** is learning to play more instruments.  
**Policy evolution** is learning to understand music itself.

One path accumulates skills.  
The other develops **general intelligence**.

---

MIT License | Built with Claude
